{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKjYQjxg7qQK"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from dataclasses import dataclass\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XI8zAfoVFAWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getwandbrun(cfgs):\n",
        "  wandb.login(key=cfgs.WANDBAPI_KEY)\n",
        "  run = wandb.init(\n",
        "      entity=\"ajheshbasnet-kpriet\",\n",
        "      project=\"RLVR\",\n",
        "      name = \"rewards-runs\",\n",
        "      config=vars(cfgs),\n",
        "  )\n",
        "  return run"
      ],
      "metadata": {
        "id": "8l8zqg6f7ys_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class configs:\n",
        "  MAX_SEQ_LEN = 512\n",
        "  REWARD_LEARNING_RATE = 1e-4\n",
        "  TRANSFORMER_LEARNING_RATE = 1e-5\n",
        "  TRAIN_LENGTH = 10000\n",
        "  VALID_LENGTH = 2000\n",
        "  DRIVE_STEP = 7_000\n",
        "  EVAL_EVERY_STEP = 1400\n",
        "  GRADIENT_ACCUM_STEPS = 8 # Increased to compensate for smaller batch size\n",
        "  MODEL_NAME = \"gpt2\"\n",
        "  WANDBAPI_KEY = \"\"\n",
        "  TRAIN_BATCH_SIZE = 8 # Reduced to save VRAM\n",
        "  VALID_BATCH_SIZE = 4\n",
        "  DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "cfg = configs()"
      ],
      "metadata": {
        "id": "IIDDwxp8hyam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(cfg.MODEL_NAME)\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "k4coeuS77FQs",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(cfg.MODEL_NAME).to(cfg.DEVICE)"
      ],
      "metadata": {
        "id": "g-QMO8xcAYnI",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpointer = torch.load(\"/content/drive/MyDrive/checkpoint_epoch_4.pth\", map_location=cfg.DEVICE)"
      ],
      "metadata": {
        "id": "qblhdMma9Mhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(checkpointer['model_state_dict'])"
      ],
      "metadata": {
        "id": "c5ACI05X9Iej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runs = getwandbrun(cfg)"
      ],
      "metadata": {
        "id": "uCzqDX0N_CAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **REWARD MODEL**"
      ],
      "metadata": {
        "id": "aYBLN2QljGau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rl_dataset = load_dataset(\"CarperAI/openai_summarize_comparisons\")"
      ],
      "metadata": {
        "id": "s2gexqfE1A_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rl_dataset"
      ],
      "metadata": {
        "id": "my2L1lUx95TV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rl_dataset_train = rl_dataset['train'].select(torch.randperm(len(rl_dataset['train'])))[:cfg.TRAIN_LENGTH]\n",
        "rl_dataset_valid = rl_dataset['valid1'].select(torch.randperm(len(rl_dataset['valid1']))[:cfg.VALID_LENGTH])"
      ],
      "metadata": {
        "id": "A7bXCfL_jM1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RewardModelDataset(Dataset):\n",
        "\n",
        "  def __init__(self, ds):\n",
        "\n",
        "    self.prompt = []\n",
        "    self.chosen = []\n",
        "    self.reject = []\n",
        "    self.tokenizer = tokenizer\n",
        "    self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "    for p, c, r in tqdm(zip(ds['prompt'], ds['chosen'], ds['rejected']), total=len(ds)):\n",
        "\n",
        "      if len(tokenizer(p)['input_ids'] + tokenizer(c)['input_ids']) <= cfg.MAX_SEQ_LEN and len(tokenizer(p)['input_ids'] + tokenizer(r)['input_ids']) <= cfg.MAX_SEQ_LEN:\n",
        "        self.prompt.append(p)\n",
        "        self.chosen.append(c)\n",
        "        self.reject.append(r)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.prompt)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    prompt_chosen = f'{self.prompt[index]}\\nTL;DR:  {self.chosen[index]}'\n",
        "    prompt_reject = f'{self.prompt[index]}\\nTL;DR:  {self.reject[index]}'\n",
        "\n",
        "    prompt_chosen_ids = self.tokenizer(prompt_chosen, max_length=cfg.MAX_SEQ_LEN-1, return_tensors = 'pt', truncation=True, padding='max_length')['input_ids'][0]\n",
        "    prompt_chosen_msk = self.tokenizer(prompt_chosen, max_length=cfg.MAX_SEQ_LEN-1, return_tensors = 'pt', truncation=True, padding='max_length')['attention_mask'][0]\n",
        "\n",
        "    prompt_chosen_ids = torch.cat((prompt_chosen_ids, torch.tensor([tokenizer.eos_token_id])))\n",
        "    prompt_chosen_ids = torch.cat((prompt_chosen_msk, torch.tensor([1])))\n",
        "\n",
        "    prompt_reject_ids = self.tokenizer(prompt_reject, max_length=cfg.MAX_SEQ_LEN-1, return_tensors = 'pt',  truncation=True, padding='max_length')['input_ids'][0]\n",
        "    prompt_reject_msk = self.tokenizer(prompt_reject, max_length=cfg.MAX_SEQ_LEN-1, return_tensors = 'pt', truncation=True, padding='max_length')['attention_mask'][0]\n",
        "\n",
        "    prompt_reject_ids = torch.cat((prompt_reject_ids, torch.tensor([tokenizer.eos_token_id])))   # eos token id is used to\n",
        "    prompt_reject_msk = torch.cat((prompt_reject_msk, torch.tensor([1])))\n",
        "\n",
        "    return prompt_chosen_ids, prompt_chosen_msk, prompt_reject_ids, prompt_reject_msk"
      ],
      "metadata": {
        "id": "ck1DhAiJjQ3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = RewardModelDataset(rl_dataset_train)\n",
        "train_loader = DataLoader(train_ds, batch_size= cfg.TRAIN_BATCH_SIZE, shuffle=True)\n",
        "\n",
        "valid1 = RewardModelDataset(rl_dataset_valid)\n",
        "valid_loader = DataLoader(valid1, batch_size=cfg.VALID_BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "U4iJp0gMpaD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RewardModel(nn.Module):\n",
        "\n",
        "  def __init__(self, model):\n",
        "    super().__init__()\n",
        "\n",
        "    self.base_model = model.transformer\n",
        "    self.reward_head = nn.Sequential(\n",
        "        nn.Linear(768, 1)\n",
        "        )\n",
        "\n",
        "  def forward(self, x, attn_mask):\n",
        "    h_s = self.base_model(input_ids = x, attention_mask=attn_mask).last_hidden_state\n",
        "    end_tk = h_s[:, -1, :]\n",
        "    rewards = self.reward_head(end_tk)\n",
        "    return rewards"
      ],
      "metadata": {
        "id": "PKDX7gWrrt7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewardmodel = RewardModel(model).to(cfg.DEVICE)\n",
        "rewardmodel = torch.compile(rewardmodel)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "D7u_9hQd1sFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "REWARD_EPOCHS = 4\n",
        "\n",
        "OPTIMIZER = torch.optim.AdamW([\n",
        "    {\"params\": rewardmodel.base_model.parameters(), \"lr\": cfg.TRANSFORMER_LEARNING_RATE},   # it has pretty decent knowledge so it's lr is less\n",
        "    {\"params\": rewardmodel.reward_head.parameters(), \"lr\": cfg.REWARD_LEARNING_RATE},  # this is new so it trains very fast hence it has high learning rates\n",
        "])"
      ],
      "metadata": {
        "id": "T6CHL5BA3ltJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.EVAL_EVERY_STEP = 1000\n",
        "len(train_loader)"
      ],
      "metadata": {
        "id": "u8sn2bsjKZvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "# ------------------- Performance Boost (Safe) -------------------\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.conv.fp32_precision = 'tf32'\n",
        "\n",
        "# ------------------- AMP setup -------------------\n",
        "scaler = GradScaler(device=\"cuda\")\n",
        "\n",
        "for epoch in range(REWARD_EPOCHS):\n",
        "\n",
        "    OPTIMIZER.zero_grad(set_to_none=True)\n",
        "    train_loss = 0.0\n",
        "    global_rollouts = 0\n",
        "    rewardmodel.train()\n",
        "\n",
        "    for step, batch in enumerate(train_loader):\n",
        "\n",
        "        prompt_chosen_ids, prompt_chosen_msk, prompt_reject_ids, prompt_reject_msk = batch\n",
        "\n",
        "        prompt_chosen_ids = prompt_chosen_ids.to(cfg.DEVICE, non_blocking=True)\n",
        "        prompt_chosen_msk = prompt_chosen_msk.to(cfg.DEVICE, non_blocking=True)\n",
        "        prompt_reject_ids = prompt_reject_ids.to(cfg.DEVICE, non_blocking=True)\n",
        "        prompt_reject_msk = prompt_reject_msk.to(cfg.DEVICE, non_blocking=True)\n",
        "\n",
        "\n",
        "        input_ids = torch.cat((prompt_chosen_ids, prompt_reject_ids), dim = 0)\n",
        "        attention_mask = torch.cat((prompt_chosen_msk, prompt_reject_msk), dim = 0)\n",
        "\n",
        "        with autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "\n",
        "            # chosen_rewards = rewardmodel(prompt_chosen_ids, prompt_chosen_msk)\n",
        "            # reject_rewards = rewardmodel(prompt_reject_ids, prompt_reject_msk)\n",
        "\n",
        "            reward = rewardmodel(input_ids, attention_mask)\n",
        "\n",
        "            chosen_rewards, reject_rewards = reward.chunk(2, dim = 0)\n",
        "\n",
        "            logits = chosen_rewards - reject_rewards\n",
        "\n",
        "            loss = F.binary_cross_entropy_with_logits(\n",
        "                logits,\n",
        "                torch.ones_like(logits)\n",
        "            )\n",
        "\n",
        "            loss = loss / cfg.GRADIENT_ACCUM_STEPS\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        train_loss += loss.detach()\n",
        "        global_rollouts += 1\n",
        "\n",
        "        if (step + 1) % cfg.GRADIENT_ACCUM_STEPS == 0:\n",
        "\n",
        "            scaler.unscale_(OPTIMIZER)\n",
        "            torch.nn.utils.clip_grad_norm_(rewardmodel.parameters(), 2.0)\n",
        "\n",
        "            scaler.step(OPTIMIZER)\n",
        "            scaler.update()\n",
        "            OPTIMIZER.zero_grad(set_to_none=True)\n",
        "\n",
        "            runs.log({\n",
        "                \"training-reward-loss\": train_loss.item(),\n",
        "                \"steps\": step + 1\n",
        "            })\n",
        "\n",
        "            train_loss = 0.0\n",
        "\n",
        "        # ---------------------- VALIDATION ----------------------\n",
        "\n",
        "        if (step + 1) % cfg.EVAL_EVERY_STEP == 0:\n",
        "\n",
        "            valid_loss = 0.0\n",
        "            valid_counter = 0\n",
        "\n",
        "            rewardmodel.eval()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for valid_batch in valid_loader:\n",
        "\n",
        "                    prompt_chosen_ids, prompt_chosen_msk, prompt_reject_ids, prompt_reject_msk = valid_batch\n",
        "\n",
        "                    prompt_chosen_ids = prompt_chosen_ids.to(cfg.DEVICE, non_blocking=True)\n",
        "                    prompt_chosen_msk = prompt_chosen_msk.to(cfg.DEVICE, non_blocking=True)\n",
        "                    prompt_reject_ids = prompt_reject_ids.to(cfg.DEVICE, non_blocking=True)\n",
        "                    prompt_reject_msk = prompt_reject_msk.to(cfg.DEVICE, non_blocking=True)\n",
        "\n",
        "                    val_ids = torch.cat((prompt_chosen_ids, prompt_reject_ids), dim = 0)\n",
        "                    val_msk = torch.cat((prompt_chosen_msk, prompt_reject_msk), dim = 0)\n",
        "\n",
        "                    with autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "\n",
        "                        # chosen_rewards = rewardmodel(prompt_chosen_ids, prompt_chosen_msk)\n",
        "                        # reject_rewards = rewardmodel(prompt_reject_ids, prompt_reject_msk)\n",
        "\n",
        "                        rewards = rewardmodel(val_ids,val_msk)\n",
        "\n",
        "                        chosen_rewards, reject_rewards = rewards.chunk(2, dim = 0)\n",
        "\n",
        "                        logits = chosen_rewards - reject_rewards\n",
        "\n",
        "                        loss = F.binary_cross_entropy_with_logits(\n",
        "                            logits,\n",
        "                            torch.ones_like(logits)\n",
        "                        )\n",
        "\n",
        "                    valid_loss += loss.detach().float().item()\n",
        "                    valid_counter += 1\n",
        "\n",
        "            valid_loss = valid_loss / valid_counter\n",
        "\n",
        "            runs.log({\n",
        "                \"valid-reward-loss\": valid_loss,\n",
        "                \"steps\": step + 1\n",
        "            })\n",
        "\n",
        "            rewardmodel.train()\n",
        "\n",
        "        # ---------------- Handle Final Partial Accumulation ----------------\n",
        "    if (step + 1) % cfg.GRADIENT_ACCUM_STEPS != 0:\n",
        "        scaler.unscale_(OPTIMIZER)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        scaler.step(OPTIMIZER)\n",
        "        scaler.update()\n",
        "\n",
        "        if 'scheduler' in globals():\n",
        "            OPTIMIZER.step()\n",
        "\n",
        "        OPTIMIZER.zero_grad(set_to_none=True)\n",
        "\n",
        "\n",
        "    # ---------------- Checkpoint ----------------\n",
        "\n",
        "    if  global_rollouts%cfg.DRIVE_STEP==0:\n",
        "      checkpoint = {\n",
        "          'epoch': epoch + 1,\n",
        "          'model_state_dict': rewardmodel.state_dict(),\n",
        "          'optimizer_state_dict': OPTIMIZER.state_dict(),\n",
        "          'scaler_state_dict': scaler.state_dict()\n",
        "      }\n",
        "\n",
        "      save_dir = \"/content/drive/MyDrive/reward-optimizer\"\n",
        "      os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "      filename = f\"checkpoint_epoch_{epoch+1}.pth\"\n",
        "      torch.save(checkpoint, os.path.join(save_dir, filename))\n",
        "      print(f\"==================== EPOCH {epoch+1} CHECKPOINTER IS SAVED ====================\")\n"
      ],
      "metadata": {
        "id": "fla3Hg-52VHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewardmodel(pc, pcm)"
      ],
      "metadata": {
        "id": "KdegbjMdiKP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewardmodel(pr, prm)"
      ],
      "metadata": {
        "id": "cFSJGqAliqqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x-uxqKo_75e0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}