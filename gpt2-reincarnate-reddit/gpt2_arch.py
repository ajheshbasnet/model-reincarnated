# -*- coding: utf-8 -*-
"""gpt2-arch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PQ6Yo8qd6HzfpA6kHPUUqELSTvyWlBr9
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F

from google.colab import drive
drive.mount('/content/drive')

class gpt2Config:

    def __init__(self):

        self.vocab_size = 50257
        self.d_model = 256
        self.n_heads = 4
        self.n_decoder = 4
        self.hidden_states = 1024
        self.dropout_p = 0.2
        self.block_size = 700
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

config = gpt2Config()

class Embedding(nn.Module):

    def __init__(self, config):

        super().__init__()

        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model)
        self.position_embedding = nn.Embedding(config.block_size, config.d_model)

    def forward(self, x):
        B, T = x.size()          # B, T
        tok_embedding = self.token_embedding(x)   # B,T,C

        pos_range = torch.arange(0, T, device = x.device)   # T
        pos_embedding = self.position_embedding(pos_range)  # T, C

        return tok_embedding + pos_embedding

class HeadAttention(nn.Module):

    def __init__(self, config):
        super().__init__()

        self.config = config
        self.attn_dropout = nn.Dropout(config.dropout_p)
        self.q_mat = nn.Linear(config.d_model, config.d_model, bias = False)
        self.k_mat = nn.Linear(config.d_model, config.d_model, bias = False)
        self.v_mat = nn.Linear(config.d_model, config.d_model, bias = False)
        self.out_proj = nn.Linear(config.d_model, config.d_model, bias = False)


    def forward(self, x):

        B, T, C = x.size()

        que = self.q_mat(x)    # B, T, C
        key = self.k_mat(x)
        val = self.v_mat(x)

        assert self.config.d_model % self.config.n_heads == 0, "d_model should be divided by n_heads"

        n_que = que.view(B, T, self.config.n_heads, C//self.config.n_heads).permute(0, 2, 1, 3)   # B, n_heads, T, d_heads
        n_key = key.view(B, T, self.config.n_heads, C//self.config.n_heads).permute(0, 2, 1, 3)
        n_val = val.view(B, T, self.config.n_heads, C//self.config.n_heads).permute(0, 2, 1, 3)

        attn_score = n_que @ n_key.transpose(-2, -1) * (1.0 / math.sqrt(C//self.config.n_heads))

        casual_mask = torch.triu(torch.ones(T, T, device = x.device), diagonal = 1)
        casual_masked = casual_mask.masked_fill(casual_mask==1, float('-inf'))

        attn_score = attn_score + casual_masked

        attn = F.softmax(attn_score, dim = -1)

        attn = self.attn_dropout(attn)

        attn = attn @ n_val     # B, n_heads, T, d_heads

        attn = attn.permute(0, 2, 1, 3).reshape(B, T, -1)

        attn = self.out_proj(attn)

        return attn

class LayerNormalization(nn.Module):

    def __init__(self, config):

        super().__init__()
        self.beta  = nn.Parameter(torch.zeros(config.d_model))
        self.gamma = nn.Parameter(torch.ones(config.d_model))

    def forward(self, x, eps=1e-5):

        B, T, C = x.size()
        mean = torch.mean(x, dim = -1, keepdim = True)    # B, T, 1
        std = torch.std(x, dim = -1, keepdim = True, unbiased=False)

        Z = (x - mean) / (std + eps)
        return Z * self.gamma + self.beta

class FFNN(nn.Module):

    def __init__(self, config):

        super().__init__()

        self.layer1 = nn.Linear(config.d_model, 4 * config.d_model, bias = False)
        self.ffnn_dropout = nn.Dropout(config.dropout_p)
        self.layer2 = nn.Linear(4 * config.d_model, config.d_model, bias = False)

    def forward(self, x):
        return self.layer2(self.ffnn_dropout(F.gelu(self.layer1(x), approximate='tanh')))

class OneDecoder(nn.Module):

    def __init__(self, config):

        super().__init__()

        self.mha = HeadAttention(config)
        self.layer_norm1 = LayerNormalization(config)
        self.ffnn = FFNN(config)
        self.layer_norm2 = LayerNormalization(config)

    def forward(self, x):
        x = self.layer_norm1(x)
        x = x + self.mha(x)

        x = self.layer_norm2(x)
        x = x + self.ffnn(x)

        return x

class DecoderStack(nn.Module):

    def __init__(self, config):

        super().__init__()

        self.layernorm = LayerNormalization(config)
        self.n_Decoders = nn.ModuleList([OneDecoder(config) for _ in range(config.n_decoder)])

    def forward(self, x):

        for layer in self.n_Decoders:
            x = layer(x)
        hidden_states = self.layernorm(x)
        return hidden_states

def init_weights(module):
    if isinstance(module, nn.Linear):
        nn.init.normal_(module.weight, mean=0.0, std=0.02)
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    elif isinstance(module, nn.Embedding):
        nn.init.normal_(module.weight, mean=0.0, std=0.02)
    elif isinstance(module, LayerNormalization):
        nn.init.ones_(module.gamma)
        nn.init.zeros_(module.beta)

class GPT2(nn.Module):

    def __init__(self, config):

        super().__init__()

        self.Embeddings = Embedding(config)
        self.decoder_stack = DecoderStack(config)
        self.projection_layer = nn.Linear(config.d_model, config.vocab_size, bias = False)

    def forward(self, x, target = None):

        x = self.Embeddings(x)
        hidden_states = self.decoder_stack(x)
        logits = self.projection_layer(hidden_states)

        loss = None
        if target is not None:                                             # if we have target then we will compute loss.
            loss = F.cross_entropy(
                logits.view(-1, logits.size(-1)),
                target.view(-1),                                       # entropy = -log(target_index_of_logits) other rest will be 0
                ignore_index = tokenizer.encode(tokenizer.pad_token)[0]
            )
        return logits, loss


    def generate(self, ids, max_new_token: int, temp: float = 1.0):

        with torch.no_grad():
          ids = ids[:, :config.block_size] if ids.size(1) > config.block_size else ids  # truncating so that the ids don't exceed blocksize.
          for _ in range(max_new_token):
              logits, _ = self(ids)
              logits = logits[:, -1, :] / temp
              norm_logits = F.softmax(logits, dim = -1)
              o_token_index = torch.multinomial(norm_logits, 1)
              ids = torch.cat((ids, o_token_index), dim = -1)
        return ids


    def calculate_log_probs(self, completion_in, completion_out):
        output_logits, _ = self(completion_in)  # B, T, C
        log_probs = F.log_softmax(output_logits, dim = -1)
        choosen_log_probs = log_probs.gather(-1, completion_out.unsqueeze(-1)).squeeze(-1)   # --> .gather(which_dimension_of output_logits, which index)
        return choosen_log_probs

gpt2 = GPT2(config)
gpt2.apply(init_weights)

print(f'{sum(p.numel() for p in gpt2.parameters())/1e6 :.2f} MILLIONS PARAMETER!!')

"""# **DATASETS PART**"""

!pip install -U transformers

from transformers import GPT2Tokenizer
from datasets import load_dataset

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

tokenizer

dataset = load_dataset('CarperAI/openai_summarize_tldr')
print(dataset)

training_dataset = dataset['train']
validation_dataset = dataset['valid']

training_dataset = training_dataset.select(range(80000))

def tokenizing(ds):
    tokenized = tokenizer(ds['prompt'])
    return tokenized

train_tok_dataset = training_dataset.map(tokenizing, batched = True, remove_columns = training_dataset.column_names)
valid_tok_dataset = validation_dataset.map(tokenizing, batched = True, remove_columns = training_dataset.column_names)

train_tok_dataset

tokenizer.encode(tokenizer.pad_token)

from torch.utils.data import Dataset, DataLoader


class MyCustomDataset(Dataset):

    def __init__(self, ds):

        self.input_ids = ds['input_ids']

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        tokens = self.input_ids[idx]

        if len(tokens) > config.block_size+1:
          tokens = tokens[:config.block_size+1]

        else:
          tokens = tokens + [50256] * (config.block_size+1 - len(tokens))

        tokens = torch.tensor(tokens, dtype = torch.long, device = config.device)

        input_tokens = tokens[:-1]
        output_tokens = tokens[1:]

        return {
            "x": input_tokens,
            "y": output_tokens
        }

TrainDataset = MyCustomDataset(train_tok_dataset)
ValidDataset = MyCustomDataset(valid_tok_dataset)

train_data = DataLoader(TrainDataset, batch_size = 8, shuffle = True) #, num_workers = 2)
valid_data = DataLoader(ValidDataset, batch_size = 8, shuffle = False)

gpt2 = gpt2.to(config.device)

optimizer = torch.optim.AdamW(gpt2.parameters(), lr = 1e-3)

scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=800, eta_min=1e-5)

from tqdm import tqdm

from torch.amp import autocast, GradScaler
import os

scaler = GradScaler(device="cuda")

total_steps = 9
training_losses = []
validation_losses = []

for step in range(total_steps):
    train_loss, train_size = 0, 0
    valid_loss, valid_size = 0, 0

    gpt2.train()
    for batch in tqdm(train_data):
        x, y = batch['x'].to(config.device), batch['y'].to(config.device)

        optimizer.zero_grad()

        # forward + loss in autocast
        with autocast(device_type="cuda"):
            logits, loss = gpt2(x, y)

        if loss is None:
            raise ValueError("Loss is None! Check targets.")

        # scale and backpropagate
        scaler.scale(loss).backward()

        # unscale before gradient clipping
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(gpt2.parameters(), max_norm=1.0)

        # optimizer step (only once per batch)
        scaler.step(optimizer)
        scaler.update()

        # scheduler step
        scheduler.step()

        train_loss += loss.item() * x.size(0)
        train_size += x.size(0)

    save_path = '/content/drive/MyDrive/gpt2_checkpoints/'
    os.makedirs(save_path, exist_ok=True)

    # save model weights
    name = f'model_weights{step}.pt'
    torch.save(gpt2.state_dict(), os.path.join(save_path, name))
    print("Model weights saved to Google Drive!")

    # validation
    gpt2.eval()
    with torch.no_grad():
        for batch in tqdm(valid_data):
            x, y = batch['x'].to(config.device), batch['y'].to(config.device)
            with autocast(device_type="cuda"):
                logits, loss = gpt2(x, y)
            valid_loss += loss.item() * x.size(0)
            valid_size += x.size(0)

    training_losses.append(train_loss / train_size)
    validation_losses.append(valid_loss / valid_size)

    print(f"Step {step+1}/{total_steps} | Train Loss: {train_loss/train_size :.2f} | Valid Loss: {valid_loss/valid_size:.2f}")

import matplotlib.pyplot as plt
import seaborn as sns

sns.set_theme(style="whitegrid", font_scale=1.2)
plt.plot(training_losses, marker = 'o')

plt.xlabel("step")
plt.ylabel("loss")
plt.grid(True, alpha = 0.2)
plt.show()

torch.save(gpt2.state_dict(), 'model_weights.pt')

import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

import torch
import os

# create a folder in Drive (optional)
save_path = '/content/drive/MyDrive/gpt2_checkpoints/'
os.makedirs(save_path, exist_ok=True)

# save model weights
torch.save(gpt2.state_dict(), os.path.join(save_path, 'model_weights.pt'))
print("Model weights saved to Google Drive!")

"""# **INFERENCE**"""

gpt2 = GPT2(config)

weights_path = "/content/drive/MyDrive/gpt2_checkpoints/model_weights.pt"

state_dict = torch.load(weights_path, map_location = config.device)
gpt2.load_state_dict(state_dict)

_ = gpt2.eval()
_ = gpt2.to(config.device)

def generate_texts(text: str, model, max_new_token, temp = 1.0):
  token_ids = tokenizer.encode(text)
  token_ids = torch.tensor(token_ids, dtype = torch.long).to(config.device).unsqueeze(0)
  response_id = model.generate(token_ids, max_new_token, temp)
  return print(f'Generated Texts:\n{'=' * 16}\n\n{tokenizer.decode(list(response_id[0]), skip_special_tokens = True)}')

#I will lift my private bus today and this awesome girl I've gone to private bus today and this awesome girl I've gone to

query = "SUBREDDIT: r/relationships"
generate_texts(query, gpt2, 60)

